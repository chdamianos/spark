{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By using parallelize( ) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n",
      "|col1|col2|col3| col4|\n",
      "+----+----+----+-----+\n",
      "|   1|   2|   3|a b c|\n",
      "|   4|   5|   6|d e f|\n",
      "|   7|   8|   9|g h i|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"Python Spark create RDD example\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "df = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'),\n",
    "                                    (4, 5, 6, 'd e f'),\n",
    "                                    (7, 8, 9, 'g h i')]) \\\n",
    "                        .toDF(['col1', 'col2', 'col3','col4'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myData = spark.sparkContext.parallelize([(1,2), (3,4), (5,6), (7,8), (9,10)])\n",
    "myData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By using createDataFrame( ) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------------+\n",
      "| Id| Name|Sallary|DepartmentId|\n",
      "+---+-----+-------+------------+\n",
      "|  1|  Joe|  70000|           1|\n",
      "|  2|Henry|  80000|           2|\n",
      "|  3|  Sam|  60000|           2|\n",
      "|  4|  Max|  90000|           1|\n",
      "+---+-----+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Employee = spark.createDataFrame([('1', 'Joe','70000', '1'),\n",
    "                                    ('2', 'Henry','80000', '2'),\n",
    "                                    ('3', 'Sam','60000', '2'),\n",
    "                                    ('4', 'Max','90000', '1')],\n",
    "                                ['Id', 'Name','Sallary','DepartmentId']\n",
    ")\n",
    "\n",
    "Employee.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using read and load functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read dataset from .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+-----+\n",
      "|_c0|   TV|Radio|Newspaper|Sales|\n",
      "+---+-----+-----+---------+-----+\n",
      "|  1|230.1| 37.8|     69.2| 22.1|\n",
      "|  2| 44.5| 39.3|     45.1| 10.4|\n",
      "|  3| 17.2| 45.9|     69.3|  9.3|\n",
      "|  4|151.5| 41.3|     58.5| 18.5|\n",
      "|  5|180.8| 10.8|     58.4| 12.9|\n",
      "+---+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv').\\\n",
    "options(header='true', \\\n",
    "inferschema='true').\\\n",
    "load(\"./data/Advertising.csv\", header=True)\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read dataset from DataBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Reading tables from Database needs the proper drive for the corresponding Database. For example,\n",
    "the above demo needs org.postgresql.Driver and you need to download it and put it in jars folder\n",
    "of your spark installation path. I download postgresql-42.1.1.jar from the official website and put\n",
    "it in jars folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## User information\n",
    "# user = 'your_username'\n",
    "# pw\n",
    "# = 'your_password'\n",
    "# ## Database information\n",
    "# table_name = 'table_name'\n",
    "# url = 'jdbc:postgresql://##.###.###.##:5432/dataset?user='+user+'& password='+pw\n",
    "# properties ={'driver': 'org.postgresql.Driver', 'password': pw,'user': user}\n",
    "# df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
    "# df.show(5)\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read dataset from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",TV,Radio,Newspaper,Sales\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "sc = SparkContext.getOrCreate();\n",
    "hc = HiveContext(sc)\n",
    "tf1 = sc.textFile(\"./data/Advertising.csv\")\n",
    "print(tf1.first())\n",
    "# hc.sql(\"use root\")\n",
    "# spf = hc.sql(\"SELECT * FROM ads LIMIT 100\")\n",
    "# print(spf.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rdd.DataFrame vs pd.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas\n",
      "   A  B  C\n",
      "0  a  1  2\n",
      "1  b  2  3\n",
      "2  c  3  4\n",
      "spark\n",
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  a|  1|  2|\n",
      "|  b|  2|  3|\n",
      "|  c|  3|  4|\n",
      "+---+---+---+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "my_list = [['a', 1, 2], ['b', 2, 3],['c', 3, 4]]\n",
    "col_name = ['A', 'B', 'C']\n",
    "\n",
    "print('pandas')\n",
    "print(pd.DataFrame(my_list,columns= col_name))\n",
    "print('spark')\n",
    "print(spark.createDataFrame(my_list, col_name).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas\n",
      "   A  B  C\n",
      "0  0  1  1\n",
      "1  1  0  0\n",
      "2  0  1  0\n",
      "spark\n",
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  0|  1|  1|\n",
      "|  1|  0|  0|\n",
      "|  0|  1|  0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = {'A': [0, 1, 0],\n",
    "'B': [1, 0, 1],\n",
    "'C': [1, 0, 0]}\n",
    "\n",
    "print('pandas')\n",
    "print(pd.DataFrame(d))\n",
    "# Tedious for PySpark\n",
    "print('spark')\n",
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
